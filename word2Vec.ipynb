{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import gzip\n",
    "import numpy as np\n",
    "import gensim\n",
    "from hierarchy_label import get_one_word, id_labels, main_word2vec, valid_one_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def load_pre_trained_vector(weight_filename):\n",
    "    words = []\n",
    "    vectors = []\n",
    "    for l in open(weight_filename):\n",
    "        t = l.strip().split()\n",
    "        words.append(t[0])\n",
    "        vectors.append(list(map(float, t[1:])))\n",
    "    wordvecs = np.array(vectors, dtype=np.double)\n",
    "    word2id = {word:i for i, word in enumerate(words)}\n",
    "    word2vec = {word:vectors[i] for i, word in enumerate(words)}\n",
    "    return word2vec\n",
    "main_word2vec = load_pre_trained_vector(\"glove.6B.50d.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "400000\n"
     ]
    }
   ],
   "source": [
    "print len(main_word2vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def check_availibility(word_list):\n",
    "    not_in_vocab = list()\n",
    "    for word in word_list:\n",
    "        try:\n",
    "            x = get_vec(word)\n",
    "        except:\n",
    "            #print \"not all words in the list of words are in the vocavulary list\"\n",
    "            not_in_vocab.append(word)\n",
    "    return not_in_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'google_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-9a3bd82eb48e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mgoogle_word2vec\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwv\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mgoogle_word2vec\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mgoogle_word2vec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_google_word2vec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgoogle_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'google_model' is not defined"
     ]
    }
   ],
   "source": [
    "# Load Google's pre-trained Word2Vec model.\n",
    "google_model = gensim.models.KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin.gz', binary=True)  \n",
    "def get_google_word2vec(model):\n",
    "    google_word2vec = dict()\n",
    "    vocab = model.vocab.keys()\n",
    "    for word in vocab:\n",
    "        google_word2vec[word.lower()] = model.wv[word]\n",
    "    return google_word2vec\n",
    "google_word2vec = get_google_word2vec(google_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def get_vec(word, model_dict = main_word2vec):\n",
    "    try:\n",
    "        return np.array(model_dict[word])\n",
    "    except KeyError:\n",
    "        print \"label \\\"{0}\\\" is not in the vocabulary list\".format(word)\n",
    "        raise KeyError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def similarity_score(this, that, normalized = True):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        this - a numpy array representing the first vector \n",
    "        that - a numpy array representing the second vector \n",
    "        normalized - indicates if the score needs to be normalized\n",
    "    \"\"\"\n",
    "    if normalized:\n",
    "        score = np.dot(this, that)/(np.linalg.norm(this)*np.linalg.norm(that))\n",
    "    else:\n",
    "        score = np.dot(this, that)\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def nearest(vec, ve):\n",
    "    vnorm = norm(vec)\n",
    "    scores = []\n",
    "    for i in range(len(words)):\n",
    "        wvnorm = norm(wordvecs[i])\n",
    "        if not dot:\n",
    "            scores.append(np.dot(wordvecs[i], vec) / (vnorm * wvnorm))\n",
    "        else:\n",
    "            scores.append(np.dot(wordvecs[i], vec))\n",
    "    score_ids = [(s, i) for i, s in enumerate(scores)]\n",
    "    score_ids.sort()\n",
    "    score_ids.reverse()\n",
    "    return score_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.756947340987\n"
     ]
    }
   ],
   "source": [
    "print similarity_score(\"mit\",\"harvard\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.25364, 1.6843, -0.31707, -1.4498, -1.0986, -0.61929, -0.76222, -0.199, -0.56629, -0.36648, -0.42876, -0.40423, -0.33675, 0.81732, -1.7368, 0.83532, -1.3236, -0.0083179, 0.33661, 2.028, -0.53386, 1.068, -0.50475, 0.6497, -0.28377, -0.27978, -0.21748, 0.31187, -0.57689, 0.040016, -0.20781, 0.35983, 0.16429, 0.76989, -0.11843, -0.89244, 0.74802, -0.41478, 1.6046, 0.5182, 0.8655, -1.2776, -0.2592, 0.80782, 1.1082, 1.4313, 0.45357, 0.75705, -0.68866, 0.43962]\n"
     ]
    }
   ],
   "source": [
    "print get_vec(\"newyork\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  7.75146484e-03  -3.24707031e-02  -1.31225586e-02   3.17382812e-02\n",
      "   6.37817383e-03   5.26428223e-04   9.46044922e-04   1.97753906e-02\n",
      "   1.73339844e-02   4.07714844e-02  -4.05883789e-03  -8.30078125e-02\n",
      "  -6.68945312e-02   6.88476562e-02   1.25122070e-02   6.64062500e-02\n",
      "   5.46875000e-02   2.00195312e-02  -1.07421875e-02  -1.73339844e-02\n",
      "  -2.89306641e-02   2.94189453e-02   1.13769531e-01  -6.17675781e-02\n",
      "  -8.11767578e-03  -4.22363281e-02  -2.44140625e-03   8.44726562e-02\n",
      "   2.62451172e-02   3.49426270e-03   6.73828125e-02   6.98242188e-02\n",
      "  -6.50024414e-03   7.08007812e-02  -5.00488281e-03  -5.98144531e-02\n",
      "   2.31933594e-02  -3.95507812e-02   2.25830078e-02  -7.91015625e-02\n",
      "  -4.49218750e-02  -1.77001953e-02   2.67333984e-02  -4.27246094e-02\n",
      "  -1.40991211e-02  -3.06396484e-02  -1.28173828e-02  -5.02929688e-02\n",
      "  -1.25732422e-02   1.67236328e-02  -6.59179688e-02  -3.71093750e-02\n",
      "   4.78515625e-02   2.60009766e-02   5.12695312e-02   3.46679688e-02\n",
      "  -2.07519531e-02  -7.12890625e-02   2.25830078e-02   2.57568359e-02\n",
      "   2.18505859e-02  -1.19628906e-02   5.22460938e-02  -2.08740234e-02\n",
      "   7.24792480e-04   5.41992188e-02  -8.34960938e-02  -7.86132812e-02\n",
      "   4.58984375e-02   1.77001953e-02  -4.36401367e-03   1.27792358e-04\n",
      "  -2.22167969e-02   4.76074219e-02  -1.16699219e-01  -1.66015625e-02\n",
      "   7.86132812e-02  -2.57568359e-02   3.32641602e-03   5.73730469e-02\n",
      "  -1.90429688e-02   2.14843750e-02  -1.92871094e-02  -6.98242188e-02\n",
      "  -2.63671875e-02  -3.02734375e-02  -5.81054688e-02   2.51464844e-02\n",
      "  -9.32617188e-02   5.32226562e-02  -4.95605469e-02   5.24902344e-03\n",
      "  -2.83203125e-02  -2.62451172e-02  -5.18798828e-03  -9.61914062e-02\n",
      "   8.85009766e-03  -3.88183594e-02   3.71093750e-02  -1.27929688e-01\n",
      "  -6.37817383e-03   1.09375000e-01   7.56835938e-02  -6.25000000e-02\n",
      "  -2.13623047e-02  -1.06811523e-02   3.34472656e-02   8.00781250e-02\n",
      "   2.36816406e-02   1.83105469e-02   5.85937500e-02   2.86865234e-02\n",
      "  -3.14941406e-02   6.98242188e-02   4.51660156e-02   3.22265625e-02\n",
      "   1.23291016e-02  -7.17773438e-02   6.93359375e-02   5.88378906e-02\n",
      "   4.88281250e-03  -4.61425781e-02   2.40478516e-02   2.94494629e-03\n",
      "   4.54101562e-02   6.07910156e-02  -5.85937500e-02   6.29882812e-02\n",
      "   4.80957031e-02  -2.90527344e-02  -2.13623047e-03  -1.86767578e-02\n",
      "  -2.49023438e-02   7.75146484e-03  -6.28662109e-03   4.12597656e-02\n",
      "  -2.66113281e-02   3.71093750e-02   4.90722656e-02  -6.07910156e-02\n",
      "   2.19726562e-02   1.48315430e-02  -1.53808594e-02  -5.71289062e-02\n",
      "  -2.05078125e-02   3.44848633e-03  -4.80957031e-02   7.03125000e-02\n",
      "   4.19921875e-02  -9.57031250e-02   5.83496094e-02  -1.36718750e-02\n",
      "   1.01928711e-02  -4.63867188e-03  -2.56347656e-02  -4.83398438e-02\n",
      "  -2.33154297e-02  -2.11181641e-02  -3.54003906e-02  -2.44140625e-02\n",
      "  -7.03125000e-02   3.19824219e-02  -6.86645508e-03  -2.23388672e-02\n",
      "  -8.88824463e-04  -2.52685547e-02  -8.54492188e-03  -2.08740234e-02\n",
      "  -7.81250000e-02   1.81884766e-02  -4.51660156e-02   6.25610352e-03\n",
      "  -7.22656250e-02  -5.15136719e-02  -4.80957031e-02  -4.32128906e-02\n",
      "  -4.98046875e-02   2.29492188e-02  -4.39453125e-02  -3.80859375e-02\n",
      "  -4.05273438e-02   9.13085938e-02   4.83398438e-02  -1.28784180e-02\n",
      "   2.35595703e-02   8.05664062e-03  -9.61914062e-02  -5.78613281e-02\n",
      "   3.46679688e-02   9.13085938e-02  -7.86132812e-02   5.73730469e-03\n",
      "  -9.13085938e-02  -4.30297852e-03   5.02929688e-02   5.83496094e-02\n",
      "  -8.34960938e-02   1.06445312e-01  -1.74560547e-02   3.83300781e-02\n",
      "   6.16455078e-03   8.97216797e-03   6.93359375e-02  -4.18090820e-03\n",
      "   2.22167969e-02  -5.15136719e-02   3.34167480e-03   4.15039062e-03\n",
      "  -3.54003906e-02  -8.78906250e-03  -3.93066406e-02   4.51660156e-02\n",
      "   8.98437500e-02  -1.01562500e-01   7.20214844e-03   4.41894531e-02\n",
      "   4.42504883e-03  -4.58984375e-02  -1.98974609e-02  -4.00390625e-02\n",
      "  -3.07617188e-02  -3.61328125e-02   4.49218750e-02   4.80957031e-02\n",
      "   5.12695312e-02   3.49121094e-02   4.95605469e-02  -7.08007812e-02\n",
      "  -2.12402344e-02  -1.95312500e-02  -5.98144531e-03   2.53906250e-02\n",
      "   2.11181641e-02  -3.49121094e-02  -2.35595703e-02   4.71191406e-02\n",
      "  -3.19824219e-02   6.00585938e-02   3.00292969e-02  -5.71289062e-02\n",
      "  -2.88085938e-02  -7.37304688e-02   5.90820312e-02  -4.17480469e-02\n",
      "   2.28271484e-02  -1.58691406e-02   8.00781250e-02   3.58886719e-02\n",
      "  -3.68652344e-02  -2.78320312e-02   4.33349609e-03   7.86132812e-02\n",
      "   2.82287598e-03   6.73828125e-02  -1.13525391e-02  -4.46777344e-02\n",
      "   1.96533203e-02   2.34375000e-02  -4.85229492e-03  -6.44531250e-02\n",
      "  -4.61425781e-02   4.00390625e-02  -7.35473633e-03  -6.54296875e-02\n",
      "  -2.13623047e-02   4.56542969e-02  -2.45361328e-02  -2.94189453e-02\n",
      "  -9.22851562e-02  -1.01562500e-01  -2.22167969e-02   6.83593750e-02\n",
      "   4.27246094e-02   8.69140625e-02   9.66796875e-02   2.11181641e-02\n",
      "  -2.52685547e-02  -3.66210938e-02  -5.66406250e-02   8.88671875e-02\n",
      "  -2.07519531e-02   2.36816406e-02  -1.76239014e-03   3.54003906e-02\n",
      "   4.17480469e-02  -1.40380859e-02   1.39770508e-02   4.08935547e-03\n",
      "   3.43322754e-03  -2.62451172e-02  -5.43212891e-03  -6.59179688e-03\n",
      "  -1.21459961e-02  -5.81054688e-02  -8.44726562e-02   1.73339844e-02\n",
      "   1.16729736e-03  -3.12500000e-02   1.57165527e-03  -1.73339844e-02]\n"
     ]
    }
   ],
   "source": [
    "print google_word2vec[\"white_rabbit\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#Not using\n",
    "def count_available_synset(synset_dict):\n",
    "    count = 0\n",
    "    available_synset = list()\n",
    "    for synset in synset_dict:\n",
    "        labels = synset_dict[synset]\n",
    "        available = False\n",
    "        for label in labels:\n",
    "            if label in google_word2vec:\n",
    "                available = True\n",
    "        if available:\n",
    "            available_synset.append(synset)\n",
    "            count+=1\n",
    "    return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#Not using\n",
    "def txtToDict(labelFile):\n",
    "    idToNames = {}\n",
    "    with open(labelFile) as f:\n",
    "        for line in f:\n",
    "            i, namesString = line.split(':')\n",
    "            namesList = namesString[:-1].split(', ')\n",
    "            namesList = ['_'.join(name.split()) for name in namesList]\n",
    "            if i != '0':\n",
    "                idToNames[int(i)] = namesList\n",
    "    return idToNames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open(\"1k_synsets.txt\",\"r\") as file:\n",
    "    all_ids = file.read().split()\n",
    "def index_to_1k_id(index):\n",
    "    return all_ids[index+1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'n02102480'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index_to_1k_id(221)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-20-a7a1d7a3c2cd>, line 4)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-20-a7a1d7a3c2cd>\"\u001b[0;36m, line \u001b[0;32m4\u001b[0m\n\u001b[0;31m    CNN model\u001b[0m\n\u001b[0m            ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "dict_id_to_name_list\n",
    "dict_id_to_images\n",
    "dict_id_to_prob_dist_from_CNN\n",
    "CNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def get_synthetic_vec(probability_distribution, T):\n",
    "    '''\n",
    "    Args:\n",
    "        probability_distribution - a numpy array of size 1000 that represents the probability of being each label in \n",
    "                                   the training dataset\n",
    "        T : the number of highest probability that we will take into account when contructing the synthetic word embedding\n",
    "        \n",
    "        # not using# id - the id of the test image\n",
    "        # not using # dict_id_to_prob_dist_from_CNN - the dictionary that map image's id to the dictionary of probability distribution\n",
    "    '''\n",
    "    #probability_distribution = dict_id_to_prob_dist_from_CNN[image_id]\n",
    "    sorted_indexs = [i[0] for i in sorted(enumerate(-probability_distribution), key=lambda x:x[1])]\n",
    "    \n",
    "    highest_T_prediction = sorted_indexs[:T]\n",
    "    highest_T_probability = np.array([probability_distribution[highest_T_prediction[i]] for i in range(T)])\n",
    "    word_embedding_vectors = list()\n",
    "    for training_id in highest_T_prediction:\n",
    "        synset_id = index_to_1k_id(training_id)\n",
    "        one_word_rep = get_one_word(synset_id)[0]\n",
    "        word_embedding_vectors.append(get_vec(one_word_rep))\n",
    "    word_embedding_vectors = np.array(word_embedding_vectors)\n",
    "    normalize_factor = np.sum(highest_T_probability)\n",
    "    synthetic_word_embedding_vector = np.dot(highest_T_probability/normalize_factor, word_embedding_vectors)\n",
    "    return synthetic_word_embedding_vector\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.606487  , -0.146124  , -0.23795   ,  0.2270903 , -0.1823892 ,\n",
       "        0.925741  , -0.00731616, -0.09193   ,  0.3856276 , -0.339076  ,\n",
       "        0.08859   ,  0.668494  ,  0.620515  ,  0.6263681 , -0.416254  ,\n",
       "        0.10828777,  0.214266  ,  0.3282325 , -0.9902139 ,  0.3185007 ,\n",
       "       -0.4689004 , -0.4312135 ,  0.6122414 , -0.199096  ,  0.3367857 ,\n",
       "       -0.152856  , -0.0981686 ,  0.489107  ,  0.24816782, -0.4731839 ,\n",
       "        0.362137  , -0.1052655 ,  0.429815  ,  0.599062  , -0.168578  ,\n",
       "        0.4098324 ,  0.2453097 , -0.371068  ,  0.2395097 , -0.4367165 ,\n",
       "       -0.0708713 , -0.2097499 , -0.174057  ,  0.365245  ,  0.9148585 ,\n",
       "       -0.3496169 ,  0.319607  , -0.3315965 ,  0.3923125 ,  0.121872  ])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probability_distribution = np.array([0.001]*1000)\n",
    "get_synthetic_vec(probability_distribution, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def nearest_neighbour(label_pool, synthetic_vector, k):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        label pool - a list of synset id from which we want to predict; each synset id must have valid-one-word\n",
    "        synthetic_vector - a numpy array representing the word embedding of an image\n",
    "        k - we will return the synset ids that have k highest similarity -> will implement this later for performance reason\n",
    "    \"\"\"\n",
    "    nearest_label = None\n",
    "    highest_similarity = -10000\n",
    "    for label_id in label_pool:\n",
    "        names = id_labels[label_id]\n",
    "        for name in names:\n",
    "            if valid_one_word(name):\n",
    "                word_embed_name = get_vec(name)\n",
    "                similarity = similarity_score(synthetic_vector, word_embed_name)\n",
    "                if similarity > highest_similarity:\n",
    "                    nearest_label = label_id\n",
    "    return [nearest_label]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_list = list()\n",
    "with open(\"1k_synsets.txt\",\"r\") as training_synsets:\n",
    "    synset_ids = training_synsets.read().split()\n",
    "for synset_id in synset_ids:\n",
    "    if get_one_word(synset_id)[0] == \"entity\":\n",
    "        print \"Emergency\"\n",
    "    word_list.append(get_one_word(synset_id)[0])\n",
    "len(check_availibility(word_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Caterpillar', 'cat']"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_list = list()\n",
    "with open(\"available_hop2.txt\",\"r\") as testing_synsets:\n",
    "    synset_ids = testing_synsets.read().split()\n",
    "    \n",
    "probability_distribution = np.array([0.001]*1000)\n",
    "synthetic_vec = get_synthetic_vec(probability_distribution, 10)\n",
    "\n",
    "id_labels[nearest_neighbour(synset_ids, synthetic_vec,1)[0]] # wait what? caterpillar and cat?? 555"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Sussex spaniel']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id_labels[\"n02102480\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(get_vec(\"spaniel\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Modify this one as you like\n",
    "def get_accuracy(prob_dist_list, synset_id_list, label_pool, k_hit = 1):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        prob_dist_list - a list of prop dist of testing images\n",
    "        synset_id_list - a list of true synset id corresponded to the prop dist in 'prop_dist_list' one by one\n",
    "        label_pool - a list of synset id from which we want to predict; each synset id must have valid-one-word\n",
    "        k_hit - we will consider correct if the first 'k_hit' predicted labels contain the true synset id\n",
    "    \"\"\"\n",
    "    num_testing = len(prob_dist_list)\n",
    "    num_correct = 0.0\n",
    "    for index in range(num_testing):\n",
    "        prob_dist = prob_dist_list[index]\n",
    "        true_label = synset_id_list[index]\n",
    "        first_k_hit = nearest_neighbour(prob_dist, label_pool, k_hit)\n",
    "        if true_label in first_k_hit:\n",
    "            num_correct += 1\n",
    "    return (num_correct/num_testing)*100.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
